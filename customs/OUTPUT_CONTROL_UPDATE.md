# è¾“å‡ºè·¯å¾„æ§åˆ¶æ›´æ–°è¯´æ˜

## ğŸ¯ æ ¸å¿ƒå˜æ›´

å°† `make_custom_dataset.py` ä¿®æ”¹ä¸º**å¼ºåˆ¶è¦æ±‚æŒ‡å®šç»Ÿä¸€è¾“å‡ºç›®å½•**çš„æ¨¡å¼ï¼Œæ”¯æŒå¤šä¸ªä¸åŒè·¯å¾„çš„è¾“å…¥ï¼Œä½†æ‰€æœ‰è¾“å‡ºéƒ½æ”¾åˆ°ä¸€ä¸ªç»™å®šè·¯å¾„ä¸‹ã€‚

## ğŸ“ API å˜æ›´

### åŸæ¥çš„å‡½æ•°ç­¾åï¼š
```python
def create_dataset(data_dirs, dataloader_process_only, output_dir=None, supported_formats=['.wav', '.flac']):
```

### ç°åœ¨çš„å‡½æ•°ç­¾åï¼š
```python
def create_dataset(data_dirs, dataloader_process_only, output_dir, supported_formats=['.wav', '.flac']):
```

**å…³é”®å˜æ›´**ï¼š
- âŒ ç§»é™¤äº† `output_dir=None` çš„é»˜è®¤å€¼
- âœ… `output_dir` ç°åœ¨æ˜¯**å¿…éœ€å‚æ•°**

## ğŸ”„ ä½¿ç”¨æ–¹å¼å¯¹æ¯”

### âŒ åŸæ¥çš„ç”¨æ³•ï¼ˆä¸å†æ”¯æŒï¼‰ï¼š
```python
# è¿™äº›è°ƒç”¨ç°åœ¨ä¼šæŠ¥é”™
create_dataset(
    data_dirs="/path/to/audio",
    dataloader_process_only=True
    # output_dir æœªæŒ‡å®š - æŠ¥é”™ï¼
)

create_dataset(
    data_dirs="/path/to/audio", 
    dataloader_process_only=True,
    output_dir=None  # æ˜¾å¼è®¾ä¸º None - æŠ¥é”™ï¼
)
```

### âœ… ç°åœ¨çš„ç”¨æ³•ï¼ˆå¿…é¡»æŒ‡å®šè¾“å‡ºç›®å½•ï¼‰ï¼š
```python
# å•ç›®å½•è¾“å…¥ï¼ŒæŒ‡å®šè¾“å‡º
create_dataset(
    data_dirs="/path/to/audio",
    dataloader_process_only=True,
    output_dir="/output/my_dataset"  # å¿…é¡»æä¾›ï¼
)

# å¤šç›®å½•è¾“å…¥ï¼Œç»Ÿä¸€è¾“å‡º  
create_dataset(
    data_dirs=[
        "/data/english_audio",
        "/data/chinese_audio",
        "/data/japanese_audio"
    ],
    dataloader_process_only=True,
    output_dir="/output/multilingual_dataset"  # æ‰€æœ‰æ•°æ®è¾“å‡ºåˆ°è¿™é‡Œ
)
```

## ğŸ æ–°å¢åŠŸèƒ½

### 1. è¯¦ç»†çš„å¤„ç†é…ç½®æ˜¾ç¤º
```python
=== å¤„ç†é…ç½® ===
è¾“å…¥ç›®å½•: 3 ä¸ªç›®å½•
  1. /data/english_audio
  2. /data/chinese_audio  
  3. /data/japanese_audio
è¾“å‡ºç›®å½•: /output/multilingual_dataset
æ”¯æŒæ ¼å¼: ['.wav', '.flac']
æ‰¾åˆ°æ–‡ä»¶: 1250 ä¸ª

=== è¾“å‡ºæ–‡ä»¶ ===
HDF5æ–‡ä»¶: /output/multilingual_dataset/audio_sum.hdf5
æ³¨é‡Šæ–‡ä»¶: /output/multilingual_dataset/audio_ann_sum.txt
```

### 2. å¢å¼ºçš„é”™è¯¯å¤„ç†
```python
# å‚æ•°éªŒè¯
if not output_dir:
    raise ValueError("output_dir å‚æ•°æ˜¯å¿…éœ€çš„ã€‚è¯·æŒ‡å®šä¸€ä¸ªè¾“å‡ºç›®å½•è·¯å¾„ã€‚")

# è·¯å¾„å¤„ç†
output_dir = os.path.abspath(output_dir)

# æƒé™æ£€æŸ¥
try:
    os.makedirs(output_dir, exist_ok=True)
except PermissionError:
    raise PermissionError(f"æ— æƒé™åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
```

### 3. è¯¦ç»†çš„å¤„ç†ç»“æœè¿”å›
```python
result = create_dataset(...)

# å‡½æ•°ç°åœ¨è¿”å›è¯¦ç»†ä¿¡æ¯
{
    'h5_path': '/output/dataset/audio_sum.hdf5',
    'ann_path': '/output/dataset/audio_ann_sum.txt', 
    'processed_count': 1180,
    'failed_count': 70,
    'total_files': 1250
}
```

### 4. å¤„ç†è¿›åº¦å’Œç»Ÿè®¡æ˜¾ç¤º
```python
=== å¤„ç†å®Œæˆ ===
æˆåŠŸå¤„ç†: 1180 ä¸ªæ–‡ä»¶
å¤„ç†å¤±è´¥: 70 ä¸ªæ–‡ä»¶
æ€»å¤„ç†ç‡: 94.4%

=== è¾“å‡ºæ–‡ä»¶ ===
HDF5æ•°æ®æ–‡ä»¶: /output/dataset/audio_sum.hdf5
æ³¨é‡Šæ–‡ä»¶: /output/dataset/audio_ann_sum.txt
HDF5æ–‡ä»¶å¤§å°: 245.7 MB
æ³¨é‡Šæ–‡ä»¶è¡Œæ•°: 1180 è¡Œ
```

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### åœºæ™¯1ï¼šæ•´åˆå¤šä¸ªæ•°æ®é›†
```python
# å°†å¤šä¸ªå…¬å¼€æ•°æ®é›†æ•´åˆåˆ°ç»Ÿä¸€çš„è®­ç»ƒé›†
datasets = [
    "/datasets/librispeech/train-clean-100",
    "/datasets/librispeech/train-clean-360", 
    "/datasets/ljspeech/wavs",
    "/datasets/vctk/wav48"
]

create_dataset(
    data_dirs=datasets,
    dataloader_process_only=True,
    output_dir="/training/unified_english_corpus"
)
```

### åœºæ™¯2ï¼šé¡¹ç›®ä¸“ç”¨æ•°æ®é›†
```python
# ç ”ç©¶é¡¹ç›®çš„å¤šè¯­è¨€æ•°æ®æ•´åˆ
research_sources = [
    "/lab_data/emotions/english",
    "/lab_data/emotions/chinese", 
    "/lab_data/emotions/japanese",
    "/partner_data/speech_corpus",
    "/public_datasets/commonvoice"
]

create_dataset(
    data_dirs=research_sources,
    dataloader_process_only=True,
    output_dir="/research/emotion_speech_project/dataset_v1.0"
)
```

### åœºæ™¯3ï¼šç”Ÿäº§ç¯å¢ƒæ•°æ®ç®¡é“
```python
# ç”Ÿäº§ç¯å¢ƒçš„å¤šæºæ•°æ®èšåˆ
production_sources = [
    "/data/user_uploads/validated",
    "/data/tts_generated/high_quality",
    "/data/studio_recordings/clean",
    "/data/call_center/processed"
]

create_dataset(
    data_dirs=production_sources,
    dataloader_process_only=True,
    output_dir="/ml_models/training_data/speech_v2.1"
)
```

## ğŸ”§ æŠ€æœ¯æ”¹è¿›

### 1. è·¯å¾„å¤„ç†å¢å¼º
```python
# è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
output_dir = os.path.abspath(output_dir)

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(output_dir, exist_ok=True)

# æ™ºèƒ½æ–‡ä»¶è·¯å¾„ç”Ÿæˆ
h5_output_path = os.path.join(output_dir, "audio_sum.hdf5")
ann_output_path = os.path.join(output_dir, "audio_ann_sum.txt")
```

### 2. æ›´å¥½çš„æ–‡ä»¶åå†²çªå¤„ç†
```python
# ä½¿ç”¨ç›¸å¯¹è·¯å¾„ç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦
rel_path = os.path.relpath(audio_path)
stem = rel_path.replace('/', '_').replace('\\', '_').replace('.', '_')
stem = ''.join(c for c in stem if c.isalnum() or c in ('_', '-'))
```

### 3. è¾“å‡ºæ–‡ä»¶ä¿¡æ¯ç»Ÿè®¡
```python
# æ˜¾ç¤ºæ–‡ä»¶å¤§å°å’Œè¡Œæ•°
if os.path.exists(h5_output_path):
    h5_size = os.path.getsize(h5_output_path) / (1024*1024)  # MB
    print(f"HDF5æ–‡ä»¶å¤§å°: {h5_size:.1f} MB")

if os.path.exists(ann_output_path):
    with open(ann_output_path, 'r', encoding='utf-8') as f:
        line_count = sum(1 for _ in f)
    print(f"æ³¨é‡Šæ–‡ä»¶è¡Œæ•°: {line_count} è¡Œ")
```

## âš ï¸ è¿ç§»æŒ‡å—

### å¦‚æœä½ ä¹‹å‰è¿™æ ·ä½¿ç”¨ï¼š
```python
# è€ä»£ç 
create_dataset("/path/to/audio", True)
```

### ç°åœ¨éœ€è¦æ”¹ä¸ºï¼š
```python
# æ–°ä»£ç 
create_dataset(
    data_dirs="/path/to/audio",
    dataloader_process_only=True,
    output_dir="/path/to/output"  # å¿…é¡»æ·»åŠ ï¼
)
```

### å¤šç›®å½•çš„æƒ…å†µï¼š
```python
# è€ä»£ç ï¼ˆå‡è®¾çš„ï¼‰
for data_dir in multiple_dirs:
    create_dataset(data_dir, True)  # åˆ†åˆ«å¤„ç†

# æ–°ä»£ç ï¼ˆæ¨èï¼‰
create_dataset(
    data_dirs=multiple_dirs,        # ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰ç›®å½•
    dataloader_process_only=True,
    output_dir="/unified/output"    # ç»Ÿä¸€è¾“å‡ºä½ç½®
)
```

## ğŸ’¡ ä¼˜åŠ¿

1. **æ˜ç¡®çš„è¾“å‡ºæ§åˆ¶**ï¼šç”¨æˆ·å¿…é¡»æ˜ç¡®æŒ‡å®šè¾“å‡ºä½ç½®
2. **ç»Ÿä¸€æ•°æ®ç®¡ç†**ï¼šå¤šä¸ªæ•°æ®æºçš„è¾“å‡ºé›†ä¸­åœ¨ä¸€ä¸ªä½ç½®
3. **æ›´å¥½çš„é¡¹ç›®ç»„ç»‡**ï¼šä¾¿äºç®¡ç†å¤§å‹é¡¹ç›®çš„æ•°æ®é›†
4. **é¿å…æ„å¤–è¦†ç›–**ï¼šä¸ä¼šæ„å¤–å†™å…¥åˆ°è¾“å…¥ç›®å½•
5. **è¯¦ç»†çš„å¤„ç†åé¦ˆ**ï¼šæä¾›å®Œæ•´çš„å¤„ç†ç»“æœä¿¡æ¯

## ğŸ›¡ï¸ å‘åå…¼å®¹æ€§

è¿™æ˜¯ä¸€ä¸ª**ç ´åæ€§å˜æ›´**ï¼Œå› ä¸ºï¼š
- ç§»é™¤äº† `output_dir=None` çš„é»˜è®¤è¡Œä¸º
- æ‰€æœ‰è°ƒç”¨éƒ½å¿…é¡»æ˜¾å¼æŒ‡å®š `output_dir`

ä½†æ˜¯ï¼Œè¿™ä¸ªå˜æ›´æ˜¯**æœ‰æ„ä¸ºä¹‹**çš„ï¼Œå› ä¸ºï¼š
- æé«˜äº†APIçš„æ˜ç¡®æ€§
- é¿å…äº†æ„å¤–çš„æ–‡ä»¶å†™å…¥
- æ›´é€‚åˆç”Ÿäº§ç¯å¢ƒå’Œé¡¹ç›®ç®¡ç†

å¦‚æœéœ€è¦ä¿æŒå®Œå…¨å‘åå…¼å®¹ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªåŒ…è£…å‡½æ•°ï¼š
```python
def create_dataset_legacy(data_dirs, dataloader_process_only, output_dir=None, **kwargs):
    """å‘åå…¼å®¹çš„åŒ…è£…å‡½æ•°"""
    if output_dir is None:
        if isinstance(data_dirs, str):
            output_dir = data_dirs
        else:
            output_dir = data_dirs[0]
    
    return create_dataset(data_dirs, dataloader_process_only, output_dir, **kwargs)
```

é€šè¿‡è¿™äº›ä¿®æ”¹ï¼Œ`make_custom_dataset.py` ç°åœ¨æ›´é€‚åˆå¤„ç†å¤æ‚çš„å¤šæºæ•°æ®é›†æ•´åˆä»»åŠ¡ï¼ŒåŒæ—¶æä¾›äº†æ›´å¥½çš„æ§åˆ¶å’Œåé¦ˆæœºåˆ¶ã€‚
